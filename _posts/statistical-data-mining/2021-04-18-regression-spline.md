---
title: "Regression Spline"
layout: post
use_math: true
tags: ["Statistical Data Mining"]
---

### ì„œë¡ 
2021-1í•™ê¸°, ëŒ€í•™ì—ì„œ 'í†µê³„ì  ë°ì´í„°ë§ˆì´ë‹' ìˆ˜ì—…ì„ ë“£ê³  ê³µë¶€í•œ ë°”ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

<br><span class="statement-title">TOC.</span><br>

- Basis Expansion
- Polynomial Regression
- Regression Spline
  - spline basis function
- Natural Cubic Spline

<hr/>

ì´ì „ ì±•í„°ì—ì„œëŠ” parameterë¥¼ ì¶”ë¡ í•´ linear regreeion & linear classificationì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ì•˜ë‹¤.

$$
f(x) = E[Y \mid X = x] = \beta^T x
$$

ì´ë²ˆ ì±•í„°ì—ì„œëŠ” **non-linear model**ì— ëŒ€í•´ ì‚´í´ë³¼ ì˜ˆì •ì´ë©°, \<Basis Expansion\>ê³¼ \<Kernel Method\>ì„ ì¤‘ì‹¬ìœ¼ë¡œ non-linear modelì„ ì‚´í´ë³¸ë‹¤!

<hr/>

### Basis Expansion

<span class="statement-title">Definition.</span> Basis expansion<br>

For $m=1, \dots, M$, let $h_m: \mathbb{R}^p \rightarrow \mathbb{R}$ be the $m$-th **transformation** of $X$.

Then, we model as 

$$
f(X) = \sum^M_{m=1} \beta_m h_m (X)
$$

with a **<u>linear basis expansion</u>** in $X$.

<br/>

<span class="statement-title">Example.</span><br>

1\. feature dim $p$ë¡œ ë¶„í• 

$$
h_m(X) = X_m \quad \text{for} \quad m=1, \dots, p
$$

2\. Covariatic Transform <small>(ì •ì‹ ëª…ì¹­ì€ ì•„ë‹ˆê³ , Cov ê°™ì€ ëŠë‚Œì´ë¼ ë³¸ì¸ì€ ì´ë ‡ê²Œ ë¶€ë¥¸ë‹¤.)</small>

$$
h_m (X) = X^2_j \quad \text{or} \quad h_m(X) = X_j X_k
$$

3\. log transform or root transform

$$
h_m(X) = \log (X_j) \quad \text{of} \quad h_m (X) = \sqrt{X_j}
$$

4\. range transform

$$
h_m(X) = I(L_m \le X_k < U_m)
$$

íŠ¹ì • ë²”ìœ„ì— ë”°ë¼ $X_k$ë¥¼ ì œë‹¨í•˜ëŠ” transformì´ë‹¤.

<br/>

5\. **Splines** ğŸ”¥

ê³§ ë§Œë‚  ì˜ˆì •!

<br/>

6\. Wavelet bases

ì •ë§ ì¤‘ìš”í•˜ê²Œ ì“°ì´ëŠ” ë°©ë²•ì´ì§€ë§Œ, ì •ê·œ ìˆ˜ì—…ì—ì„œëŠ” ë‹¤ë£¨ì§€ ì•Šì•˜ë‹¤.

<hr/>

### Polynomial Regression

<div class="statement">

"Every smooth function can be approximated by polynomials!" <br/>

<small style="color: grey">-- Weierstrass's Approximation Theorem</small>

</div>

For $p$-dimensional $X$, the $m$-th order polynomial is given as

$$
f(X) = \beta_0 + \left(\sum^p_{j=1} \beta_j X_j\right) + \left(\sum^{p^2}_{j,k} \beta_{jk} X_j X_k\right) + \cdots + \left(\sum^{p^m}_{j_1, \dots, j_m} \beta_{j_1, \dots, j_m} X_{j_1} \cdots X_{jm} \right)
$$

- As $p$ increases, the # of parameters grows exponentially.
- In general, it is difficult to estimate $p$-dimentional regression function for large $p$.

ë…¼ì˜ í¸ì˜ë¥¼ ìœ„í•´ $p=1$ë¼ê³  í•˜ì. ê·¸ë¦¬ê³ , $X_i$ë¥¼ ì•„ë˜ì™€ ê°™ì´ ë””ìì¸ í•˜ì.

Let $X_1 \sim \text{Unif}(0, 1)$, and $X_m = X_1^m$ for $m \le 4$.

ì´ë•Œ, $X_1, \dots, X_4$ì˜ Corrë¥¼ êµ¬í•´ë³´ë©´,

$$
\text{Corr}(X_1, \dots, X_4) = \begin{bmatrix}
    1.00 & 0.97 & 0.92 & 0.86 \\
    0.97 & 1.00 & 0.98 & 0.95 \\
    0.92 & 0.98 & 1.00 & 0.99 \\
    0.86 & 0.95 & 0.99 & 1.00
\end{bmatrix}
$$

ì™€ ê°™ì€ë°, Corrì˜ ê°’ì„ ì‚´í´ë³´ë©´, ëª¨ë‘ 1ì— ê°€ê¹Œìš´ ê°’ì„ ê°€ì§„ë‹¤. ì´ê²ƒì€ ê° <span class="half_HL">ë…ë¦½ë³€ìˆ˜ ì‚¬ì´ì— ê°•í•œ ìƒê´€ê´€ê³„ê°€ ìˆë‹¤</span>ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë©°, ì´ê²ƒì„ \<**Multi-collinearity**; ë‹¤ì¤‘ê³µì„ ì„±\>ì´ë¼ê³  í•œë‹¤.

- High order polynomials are often **unstable** at the boundary.

â†’ bad!!! ğŸ˜¥

<hr/>

### Local Polynomial Regression

<div class="statement">

"Every smooth function can be locally approximated by low dimentional polynomials!" <br/>

<small style="color: grey">-- Talyor's Theorem</small>

</div>

Supp. that $p=1$. Divide domain interval $(a, b)$ into $K+1$ sub-intervals $(a=\xi_0, \xi_1], (\xi_1, \xi_2], \dots, (\xi_k , b=\xi_{k+1})$.

Then, a local polynomial regression model is given as

$$
f(X) = \sum^{K+1}_{i=1} f_{i}(X) \cdot I(\xi_{i-1} < X \le \xi_i)
$$

where $f_i$ are polynomials and $\xi_i$ are the **<u>knots</u>**.

ì•ì—ì„œ ì‚´í´ë³¸ \<Polynomial Regression\>ê³¼ ë¹„êµí–ˆì„ ë•Œ, <span class="half_HL">ë²”ìœ„ ì „ì²´ë¥¼ ë„ë©”ì¸ìœ¼ë¡œ ê°–ëŠ” polynomialì„ ì·¨í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ domain inteveralì„ divideí•˜ì—¬ polynomialì„ ì·¨í•œë‹¤</span>ëŠ” ì ì´ ë‹¤ë¥´ë‹¤.

<br/>

<span class="statement-title">Example.</span> Constant & Linear<br>

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/non-parametric-1.jpg" | relative_url }}" width="450px">
</div>

$f_i$ë¥¼ constant function, linear functionìœ¼ë¡œ ëª¨ë¸ë§ í–ˆì„ ë•Œì˜ ê²°ê³¼ì´ë‹¤. ê·¸ë¦¼ì—ì„œë„ ë³¼ ìˆ˜ ìˆë“¯ì´ <span class="half_HL">knots ì£¼ë³€ì—ì„œ continuous í•˜ì§€ ì•Šë‹¤</span>.

<hr/>

### Regression Spine

<span class="statement-title">Definition.</span> Regression Polynomial Spline<br>

Given a fixed knot sequence $\xi_1, \dots, \xi_K$,, a function defined on an open interval $(a, b)$ is called a \<**<u>regression (polynomial) spline of order $M$</u>**\> if it is a piecewise polynomial of order $M$ on each of the intervals.

$$
(a=\xi_0, \xi_1], (\xi_1, \xi_2], \dots, (\xi_k , b=\xi_{k+1})
$$

and <span class="half_HL">has continuous derivatives up to order $(M-1)$</span>.

ì•ì—ì„œ ì‚´í´ë³¸ \<Local Polynomial Regression\>ì˜ not-continuous ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ \<Regression Spline\>ì—ì„œëŠ” piecewise polynomialì´ $(M-1)$ì°¨ ì—°ì†ì¸ ë„í•¨ìˆ˜ë¥¼ ê°€ì ¸ì•¼ í•œë‹¤ëŠ” ì¡°ê±´ì´ ì¶”ê°€ëœ ê²ƒì´ë‹¤!!

<br/>
<hr/>

<span class="statement-title">Example.</span> 3rd-order polynomial ($M=3$)<br>

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/non-parametric-2.jpg" | relative_url }}" width="500px">
</div>

ì™¼ìª½ì€ ì•ì—ì„œ ì‚´í´ë³¸ \<Local polynomial regression\>ì˜ ëª¨ë¸ì´ë‹¤. knotì—ì„œ not-continuousí•˜ë‹¤.

ì˜¤ë¥¸ìª½ì€ <span class="half_HL">knotì—ì„œ continuousí•´ì•¼ í•œë‹¤</span>ëŠ” ì œì•½ì„ ì¤€ ëª¨ë¸ì´ë‹¤.

ì´ëŠ” $x \mapsto a_3 x^3 + a_x x^3 + a_1 x + a_0$ì¸ ê° $f_i\left((\xi_{i-1}, \xi_i]\right)$ì— ëŒ€í•´ 

$$
f_i(\xi_i) = f_{i+1}(\xi_i)
$$

ë¼ëŠ” ì¡°ê±´ì´ ì¶”ê°€ëœ ê²ƒì´ë‹¤. ì´ëŠ” ê³§, $f_i(\xi_i) = f_{i+1}(\xi_i)$ì´ê¸° ë•Œë¬¸ì—, $f_{i+1}$ì—ì„œ $b_4, b_3, b_2$ì— ëŒ€í•œ ê°’ì´ ì •í•´ì¡Œë‹¤ë©´, ìë™ìœ¼ë¡œ $b_0$ì˜ ê°’ì´ ê²°ì •ëœë‹¤.

$$
a_3 x^3 + a_2 x^2 + a_1 x + a_0 = b_3 x^3 + b_2 x^2 + b_1 x + \cancelto{\text{calculated by contraint}}{b_0}
$$

ë”°ë¼ì„œ, estimate í•´ì•¼ í•˜ëŠ” Coefficientsì˜ ê°¯ìˆ˜ëŠ”

$$
(M+1)(K+1) - (K-1)
$$

(ex: If $M=3$ and $K=2$, then we need to estimate $4 + 4 - 1$ coefficients.)

<br/>
<hr/>

<span class="statement-title">Example.</span> 3rd-order polynomial with 1st & 2nd derivative<br>

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/non-parametric-3.jpg" | relative_url }}" width="500px">
</div>

ì´ë²ˆì—ëŠ” "knotì—ì„œ continuous" ì¡°ê±´ê³¼ í•¨ê»˜ "<span class="half_HL">knotì—ì„œ 1st/2nd derivatives continuous</span>" ì¡°ê±´ì´ ì¶”ê°€ë˜ì—ˆë‹¤.

ì´ë¥¼ ë§Œì¡±í•˜ë ¤ë©´, $f_i' (\xi_i) = f_{i+1}'(\xi_i)$ë¥¼ ë§Œì¡±í•´ì•¼ í•˜ë©°, ì´ê²ƒì€ ê³§

$$
3 a_3 x^2 + 2 a_2 x + a_1 = 3 b_3 x^2 + 2 b_2 x + \cancelto{\text{calculated by contraint}}{b_1}
$$

ì´ë¯€ë¡œ contraintë¥¼ í†µí•´ $b_1$ì˜ ê°’ì„ ì •í•  ìˆ˜ ìˆë‹¤ëŠ” ë§ì´ ëœë‹¤. 2nd derivative continuousì— ëŒ€í•´ì„œë„ ë™ì¼í•œ ë°©ë²•ìœ¼ë¡œ ì ‘ê·¼í•˜ë©´ ëœë‹¤.

ë”°ë¼ì„œ, estimate í•´ì•¼ í•˜ëŠ” Coefficientsì˜ ê°¯ìˆ˜ëŠ”

$$
(M+1)(K+1) - MK = M + K + 1
$$

<br/>
<hr/>

#### Spline basis function

<span class="statement-title">Notation.</span><br>

$$
x_{+} = \begin{cases}
    x & \text{if} \quad x > 0 \\
    0 & \text{else}
\end{cases}
$$

ë˜ëŠ”

$$
(x-\xi)_{+}^M = \begin{cases}
    (x-\xi)^M & \text{if} \quad x > \xi \\
    0 & \text{else}
\end{cases}
$$

<span class="statement-title">Example.</span><br>

ë§Œì•½ $M=1$(linear) and $K=2$ë¼ë©´, **spline basis function**ì€ ì•„ë˜ì™€ ê°™ë‹¤.

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/spline-basis-function-1.jpg" | relative_url }}" width="500px">
</div>

<br/>

ë§Œì•½ $M=3$(cubic) and $K=2$ë¼ë©´, **spline basis function**ì€ ì•„ë˜ì™€ ê°™ë‹¤.

<div class="img-wrapper">
  <img src="{{ "/images/statistical-data-mining/spline-basis-function-2.jpg" | relative_url }}" width="500px">
</div>

<hr/>

### Natural Cubic Spline

ì™„ë²½í•  ê²ƒ ê°™ì€ \<Regression Spline\> ë°©ì‹ë„ ì‘ì€ ë¬¸ì œë¥¼ ê°€ì§€ê³  ìˆë‹¤. ë°”ë¡œ ì–‘ë boundaryì—ì„œ regressionì´ ì˜ ì•ˆ ëœë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ìœ„í•´ \<Natural cubic spline\>ì€ <span class="half_HL">ì–‘ëì—ì„œ linearë¡œ ëª¨ë¸ë§ í•œë‹¤<span>.

<span class="statement-title">Definition.</span> Natrual Cubic Spline<br>

A cubic spline is called a \<**natural cubic spline**\> if it is **linear** beyond the boundary knots $\xi_1$ and $\xi_K$.

<div class="img-wrapper">
  <img src="http://www.stanford.edu/class/stats202/figs/Chapter7/7.7.png" width="500px">
    <p>
  (ì¶œì²˜: <a href="http://web.stanford.edu/class/stats202/notes/Nonlinear/Splines.html">Stanford - STAT202</a>)
  </p>
</div>

ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ë†’ì€ ì°¨ì›ì˜ $M$ìœ¼ë¡œ ê·¼ì‚¬í•  ìˆ˜ë¡ \<Regression Spline\>ì€ boundaryì—ì„œ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” ê±¸ ë³¼ ìˆ˜ ìˆë‹¤.




