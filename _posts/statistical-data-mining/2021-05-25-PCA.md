---
title: "PCA"
layout: post
use_math: true
tags: ["Statistical Data Mining"]
---

### 서론
2021-1학기, 대학에서 '통계적 데이터마이닝' 수업을 듣고 공부한 바를 정리한 글입니다. 지적은 언제나 환영입니다 :)

<br><span class="statement-title">TOC.</span><br>


<hr/>

\<PCA; Principal Component Analysis\>는 차원축소(dimensionality reduction)에 주요한 기법으로 사용되는 테크닉이다.

<div class="statement" markdown="1" align="center">

✨ Goal ✨<br/>
To find low-dimensional representation of input variables which <span class="half_HL">contains most information in data</span>

</div>

<div class="img-wrapper">
  <img src="https://i.stack.imgur.com/Q7HIP.gif" width="450px">
  <p markdown="1">Image from ['stackoverflow'](https://stats.stackexchange.com/a/140579)</p>
</div>

이때, "contains most information"이란 무슨 의미일까? \<PCA\>에서는 이것을 <span class="half_HL">데이터의 "분산(variance")를 최대한 보존</span>하는 것이라고 말한다! 이렇게 데이터의 분산을 보존하는 방향을 \<**principal component direction**\>라고 하며, 우리가 \<PCA\>를 수행하기 위해 찾아야 하는 대상이다! 😁

<hr/>

#### First Principal Component

For a design matrix $X = \left[ X_1, X_2, \dots, X_p \right]$, the first principal component $Z_1$ is the noramlized linear combination:

$$
Z_1 = \phi_{11} X_1 + \cdots + \phi_{1p}X_p
$$

that has the largest variance.

by normalization, $\sum^p_{j=1} \phi_{1j}^2 = 1$; normalization을 했기 때문에, principal component의 direction이 unique하게 정의된다.

The coefficient vector $\phi_1 = (\phi_{11}, \dots, \phi_{1p})^T$ is called the "**loading vector of the first principal component**".

<br/>

Q. Given a $n\times p$ design matrix $X$, how can we estimate $\phi_1$??

<div class="math-statement" markdown="1">

Let $X = (X_1, \dots, X_p)^T$, and w.o.l.g. let's assume that $E[X] = 0$ by standarization and $\text{Var}(X) = \Sigma$.

Then, for $Z_1 = \phi_{11}X_1 + \cdots + \phi_{1p}X_p$, we want to get $\text{maximize} \; \text{Var}(Z_1)$

</div>


<hr/>

#### references

- ['ratsgo'님의 포스트](https://ratsgo.github.io/machine%20learning/2017/04/24/PCA/)