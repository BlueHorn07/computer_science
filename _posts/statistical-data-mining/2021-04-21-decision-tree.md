---
title: "Decision Tree"
layout: post
use_math: true
tags: ["Statistical Data Mining"]
---

### ì„œë¡ 
2021-1í•™ê¸°, ëŒ€í•™ì—ì„œ 'í†µê³„ì  ë°ì´í„°ë§ˆì´ë‹' ìˆ˜ì—…ì„ ë“£ê³  ê³µë¶€í•œ ë°”ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤ :)

ğŸ’¥ ì´ë²ˆ í¬ìŠ¤íŠ¸ëŠ” \<Decision Tree\>ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì •ì˜í•˜ëŠ” ë‚´ìš©ì„ ë‹¤ë£¹ë‹ˆë‹¤. DTì— ëŒ€í•œ ì •ì˜ì™€ ê°œë…ì€ ë‹¤ë¥¸ í¬ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš” ğŸ˜‰

<br><span class="statement-title">TOC.</span><br>

- DT Estimation
- DT Construction
- Splitting Rule
  - Impurity Measures
- Pruning

<hr/>

#### DT Estimation

DTë¥¼ ì´ë£¨ëŠ” ê° terminal node ë˜ëŠ” leaf nodeëŠ” DTê°€ ë¶„í• í•˜ëŠ” region $R_i$ì— ëŒ€ì‘ëœë‹¤.

DTëŠ” *regression*ê³¼ *classification* ë‘ ë¬¸ì œì—ì„œë„ ëª¨ë‘ ì ìš©í•´ë³¼ ìˆ˜ ìˆëŠ”ë°, ê° ê²½ìš°ì— ëŒ€í•œ DT estimation function $f$ëŠ” ì•„ë˜ì™€ ê°™ì´ ì •ì˜ëœë‹¤.

1\. DT estimation in regression problem

$$
\hat{f}(x) = \sum^M_{m=1} \hat{c}_m \cdot I(x\in R_m)
$$

where 

$$
\hat{c}_m = \frac{1}{n_m} \sum_{x_i \in R_m} y_i
$$

$n_m$ is the # of training data in $R_m$.

ê·¸ëƒ¥ terminal nodeì— ì†í•˜ëŠ” training dataì˜ í‰ê· ìœ¼ë¡œ estimation í•œë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤.

<br/>

2\. DT estimation in classifcation problem

$$
\hat{P}(Y=k \mid X=x) = \hat{p}_{mk} = \frac{1}{n_m} \sum_{x_i \in R_m} I(y_i = k) \quad \text{for} \quad x \in R_m
$$

Then, for given $X = x$, estiation $\hat{Y}$ is

$$
\hat{Y} = \underset{k}{\text{argmax}} \; \hat{P}(Y=k \mid X=x)
$$

ê·¸ëƒ¥ ë‹¤ìˆ˜ê²°ì— ì˜í•´ estimation í•œë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤.

<br/>
<hr/>

DTì˜ ì¥ì ì€ ì¢‹ì€ interpretation powerë¥¼ ì œê³µí•œë‹¤ëŠ” ê²ƒì´ë‹¤! ë•Œë¡œëŠ” ê²°ì •ì— ëŒ€í•œ ì´ìœ ë¥¼ ì„¤ëª…í•´ì•¼ í•˜ëŠ” ê²½ìš°ê°€ ìˆëŠ”ë°, DTëŠ” ì´ëŸ° ìƒí™©ì—ì„œ ì¢‹ì€ interpretation insightë¥¼ ì œê³µí•œë‹¤.

ex: a bank must explain reasons for rejection to a loan applicant.

<br/>
<hr/>

#### DT Construction

DTëŠ” ì•„ë˜ 4ê°€ì§€ ê³¼ì •ì„ í†µí•´ ë§Œë“¤ì–´ì§„ë‹¤.

**1\. Growing**

- Find an optimal <u>splitting rule</u> for each node and grow the tree. 
- Stop growing if <u>stopping rule</u> is satisfied.

**2\. Prunning**

- Remove nodes which increase prediction error  or which have inappropriate inference rules. <br/>
(Complexity ê°ì†Œ! ğŸ˜†)

â€» \# of terminal nodesëŠ” DT modelì˜ complexityì— ì˜í–¥ì„ ì¤€ë‹¤.

**3\. Validation**

- Validate to decide how much we prune the tree. (Overfittingê³¼ ê´€ë ¨)

**4\. Interpretation & Prediction**

- Interpret the constructed tree & Predict!

<hr/>

#### Splitting Rule

DTë¥¼ ë§Œë“œëŠ” ê° ê³¼ì •ì—ì„œ, ì•„ë˜ ë‘ ê°€ì§€ë¥¼ ê²°ì •í•´ì¤˜ì•¼ í•œë‹¤.

1. splitting variable $X_j$
2. splitting criterion

ì´ë•Œ, $X_j$ê°€ continuousì¸ì§€ categoricalì¸ì§€ì— ë”°ë¼ criterionì„ ë§Œë“œëŠ” ìƒí™©ì´ ë‹¬ë¼ì§„ë‹¤.

##### Splitting Rule: Regression

Assume that all input variables are continuous and

$$
R_1 (j, s) = \left\{ X : X_j \le s \right\} \quad \text{and} \quad R_2(j, s) = \left\{ X : X_j > s \right\}
$$

splitting crieterionì„ ì–»ê¸° ìœ„í•´ splitting variable $X_j$ì™€ split point $s$ë¥¼ ê²°ì •í•˜ëŠ” ë¬¸ì œëŠ” ì•„ë˜ì˜ ì‹ì„ í‘¸ëŠ” ê²ƒê³¼ ë™ì¹˜ë‹¤.

$$
\min_{j, s} \left[ \min_{c_1} \left( \sum_{x_i \in R_1(j, s)} (y_i - c_1)^2\right) + \min_{c_2} \left( \sum_{x_i \in R_2 (j, s)} (y_i - c_2)^2 \right) \right]
$$

ì´ë•Œ, $c_1$ê³¼ $c_2$ëŠ” $R_1(j, s)$ì™€ $R_2(j, 2)$ì— ì†í•˜ëŠ” dataì— ëŒ€í•œ í‰ê· ê°’ì´ë‹¤.

ìœ„ì˜ ì‹ì„ ì˜ ì‚´í´ë³´ë©´, ê²°êµ­ "training error"ë¥¼ ìµœì†Œí™”í•˜ëŠ” $X_j$ì™€ $s$ë¥¼ ì°¾ëŠ” ë¬¸ì œì„ì„ ë³¼ ìˆ˜ ìˆë‹¤!

##### Splitting Rule: Classification

Classificationë„ ë§ˆì°¬ê°€ì§€ë¡œ "training error"ë¥¼ ìµœì†Œí™”í•˜ëŠ” ì•„ë˜ì˜ ì‹ì„ í’€ì–´ splitting criterionì„ êµ¬í•  ìˆ˜ ìˆë‹¤.

$$
\min_{j, s} \left[ \min_{k_1} \left( \sum_{x_i \in R_1(j, s)} I(y_i \ne k_1) \right) + \min_{k_2} \left( \sum_{x_i \in R_2(j, s)} I(y_i \ne k_2) \right) \right]
$$

ë§ˆì°¬ê°€ì§€ë¡œ ì´ë•Œ, $k_1$ê³¼ $k_2$ëŠ” $R_1(j, s)$, $R_2(j, s)$ì— ì†í•˜ëŠ” dataì—ì„œ ë‹¤ìˆ˜ê²°ì— ì˜í•´ êµ¬í•œ labelì´ë‹¤.

Classification DTì—ì„œëŠ” Splittingì´ ì¢‹ì€ì§€ ì•ˆ ì¢‹ì€ì§€ \<Impurity\>ë¥¼ í†µí•´ ì¸¡ì •í•´ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ë˜ì„œ ìœ„ì˜ ê²½ìš°ëŠ” zero-one lossë¥¼ í†µí•´ criterionì„ ê²°ì •í–ˆëŠ”ë°, ì´ ë°©ì‹ì€ ê³§ \<misclassification error\>ë¼ëŠ” impurtiy measureë¥¼ ì‚¬ìš©í•œ ê²ƒê³¼ ê°™ë‹¤. ë‹¤ë¥¸ impurity measureë¥¼ ì£¼ì–´ ë™ì¼í•œ training dataë¼ë„ ë‹¤ë¥¸ DTë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤!

<hr/>

#### Impurity Measures

**1\. Misclassification Error**

$$
\frac{1}{n_m} \sum_{i \in R_m} I(y_i \ne k_m) = 1 - \hat{p}_{mk_m}
$$

**2\. Gini Index**

$$
\sum^K_{k = 1} \hat{p}_{mk} (1 - \hat{p}_{mk})
$$

**3\.** **Cross-Entropy** or **Deviance**

$$
- \sum^K_{k=1} \hat{p}_{mk} \log \hat{p}_{mk}
$$

<hr/>

#### Pruning

DTëŠ” splittingì˜ ê³„ì†í•œë‹¤ë©´, tratining errorëŠ” 0ìœ¼ë¡œ ìˆ˜ë ´í•œë‹¤. ê·¸ëŸ¬ë‚˜ ì´ê²ƒì€ overfittingì„ ì•¼ê¸°í•˜ê³ , modelì˜ generalizationì„ ë–¨ì–´ëœ¨ë¦°ë‹¤. ê·¸ë˜ì„œ nodeì´ ê°¯ìˆ˜ë¥¼ ì¤„ì´ëŠ” pruning ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.

Let $T_0$ be a large tree obtatined by splitting.

A subtree $T \subset T_0$ is any tree that can be obtained by pruning.

let $n_m = \left\| \\{ i : x_i \in R_m \\} \right\|$ and $$\displaystyle\hat{c}_m = \frac{1}{n_m} \sum_{x_i \in R_m} y_i$$.

1\. In a regression, 

$$
Q_m (T) = \frac{1}{n_m} \sum_{x_i \in R_m} (y_i - \hat{c}_m)^2
$$

2\. In a classication <small>(ver. Gini Index)</small>

$$
Q_m(T) = \sum^K_{k=1} \hat{p}_{mk} (1-\hat{p}_{mk})
$$

Then, cost complexity pruning is defined by

$$
C_{\alpha}(T) = \left(\sum^{|T|}_{m=1} n_m Q_m(T)\right) + \alpha |T|
$$

where $\alpha > 0$ is a tunning parameter.

$C_\alpha(T)$ì— ëŒ€í•œ ì‹ì„ ì˜ ì‚´í´ë³´ë©´, "training err"ì™€ "complexity panelty"ì— ëŒ€í•œ í…€ì´ ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

ì´ì œ, ìœ„ì˜ $C_\alpha(T)$ë¥¼ ê¸°ì¤€ìœ¼ë¡œ Best Pruning Subtree $T_\alpha$ë¥¼ ê²°ì •í•  ìˆ˜ ìˆë‹¤.

$$
T_\alpha = \underset{T}{\text{argmin}} \; C_{\alpha} (T) \quad \text{where} \quad T \subset T_0
$$

ìœ„ì™€ ê°™ì€ ì ‘ê·¼ë²•ì„ \<weakest link pruning\>ë¼ê³ ë„ í•œë‹¤.

tunning parameter $\alpha$ëŠ” generalization errorë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì„ ê³¨ë¼ì„œ ì‚¬ìš©í•˜ë©´ ëœë‹¤ê³  í•œë‹¤. (Cross Validationì„ í™œìš©í•˜ëŠ” ê²ƒ ê°™ë‹¤.)

<br/>
<hr/>






